{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO7GpNNWlNWk5yvrD3Cqo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangr296/ECE539/blob/main/ECE539_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_7UEkv9r5pz",
        "outputId": "4132a3e2-714f-4aa3-96d5-7c04d951643a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#import data from drive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/Shared drives/ECE539/parkinsons/parkinsons.data', 'r') as f:\n",
        "  tmp = np.genfromtxt(f,delimiter=',')\n",
        "  np.set_printoptions(precision=3,suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "cJuaiQY3kiWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "X, y = tmp[1:, 1:], tmp[1:, -7]\n",
        "X = np.delete(X, 16, 1)\n",
        "X = X.astype(float)\n",
        "\"\"\"\n",
        "index by columns\n",
        "X[0] MDVP:Fo(Hz)\n",
        "X[1] MDVP:Fhi(Hz)\n",
        "X[2] MDVP:Flo(Hz)\n",
        "X[3] MDVP:Jitter(%)\n",
        "X[4] MDVP:Jitter(Abs)\n",
        "X[5] MDVP:RAP\n",
        "X[6] MDVP:PPQ\n",
        "X[7] Jitter:DDP\n",
        "X[8] MDVP:Shimmer\n",
        "X[9] MDVP:Shimmer(dB)\n",
        "X[10] Shimmer:APQ3\n",
        "X[11] Shimmer:APQ5\n",
        "X[12] MDVP:APQ\n",
        "X[13] Shimmer:DDA\n",
        "X[14] NHR\n",
        "X[15] HNR\n",
        "X[16] RPDE\n",
        "X[17] DFA\n",
        "X[18] spread1\n",
        "X[19] spread2\n",
        "X[20] D2\n",
        "X[21] PPE\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "kjAI1aAg5cp-",
        "outputId": "ab0dcfc2-07d4-4faa-de46-6791773d9e9e"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nindex by columns\\nX[0] MDVP:Fo(Hz)\\nX[1] MDVP:Fhi(Hz)\\nX[2] MDVP:Flo(Hz)\\nX[3] MDVP:Jitter(%)\\nX[4] MDVP:Jitter(Abs)\\nX[5] MDVP:RAP\\nX[6] MDVP:PPQ\\nX[7] Jitter:DDP\\nX[8] MDVP:Shimmer\\nX[9] MDVP:Shimmer(dB)\\nX[10] Shimmer:APQ3\\nX[11] Shimmer:APQ5\\nX[12] MDVP:APQ\\nX[13] Shimmer:DDA\\nX[14] NHR\\nX[15] HNR\\nX[16] RPDE\\nX[17] DFA\\nX[18] spread1\\nX[19] spread2\\nX[20] D2\\nX[21] PPE\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "Corr = [[0] * 2 for i in range(22)]\n",
        "for i in range(22):\n",
        "  feature = X[:, i]\n",
        "  corr, _ = pearsonr(feature, y)\n",
        "  Corr[i][0] = abs(corr)\n",
        "  Corr[i][1] = i\n",
        "  print(\"Pearson's Correlation of feature\", i, \"is: %.3f\" % corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxRfIAiJm3Kd",
        "outputId": "878d07f0-9c1e-490f-fdd7-9bd9839f53d0"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson's Correlation of feature 0 is: -0.384\n",
            "Pearson's Correlation of feature 1 is: -0.166\n",
            "Pearson's Correlation of feature 2 is: -0.380\n",
            "Pearson's Correlation of feature 3 is: 0.278\n",
            "Pearson's Correlation of feature 4 is: 0.339\n",
            "Pearson's Correlation of feature 5 is: 0.267\n",
            "Pearson's Correlation of feature 6 is: 0.289\n",
            "Pearson's Correlation of feature 7 is: 0.267\n",
            "Pearson's Correlation of feature 8 is: 0.367\n",
            "Pearson's Correlation of feature 9 is: 0.351\n",
            "Pearson's Correlation of feature 10 is: 0.348\n",
            "Pearson's Correlation of feature 11 is: 0.351\n",
            "Pearson's Correlation of feature 12 is: 0.364\n",
            "Pearson's Correlation of feature 13 is: 0.348\n",
            "Pearson's Correlation of feature 14 is: 0.189\n",
            "Pearson's Correlation of feature 15 is: -0.362\n",
            "Pearson's Correlation of feature 16 is: 0.309\n",
            "Pearson's Correlation of feature 17 is: 0.232\n",
            "Pearson's Correlation of feature 18 is: 0.565\n",
            "Pearson's Correlation of feature 19 is: 0.455\n",
            "Pearson's Correlation of feature 20 is: 0.340\n",
            "Pearson's Correlation of feature 21 is: 0.531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort all the correlations\n",
        "num = 22\n",
        "Corr.sort()\n",
        "print(\"ranking the %i lowest correlations: \" %num)\n",
        "for i in range(num):\n",
        "  print(\"feature: %2d corr: %.3f\" %(Corr[i][1], Corr[i][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtMqUMGxqw6K",
        "outputId": "b343f0de-a74d-4c00-90e6-88e8316db3ee"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ranking the 22 lowest correlations: \n",
            "feature:  1 corr: 0.166\n",
            "feature: 14 corr: 0.189\n",
            "feature: 17 corr: 0.232\n",
            "feature:  7 corr: 0.267\n",
            "feature:  5 corr: 0.267\n",
            "feature:  3 corr: 0.278\n",
            "feature:  6 corr: 0.289\n",
            "feature: 16 corr: 0.309\n",
            "feature:  4 corr: 0.339\n",
            "feature: 20 corr: 0.340\n",
            "feature: 13 corr: 0.348\n",
            "feature: 10 corr: 0.348\n",
            "feature:  9 corr: 0.351\n",
            "feature: 11 corr: 0.351\n",
            "feature: 15 corr: 0.362\n",
            "feature: 12 corr: 0.364\n",
            "feature:  8 corr: 0.367\n",
            "feature:  2 corr: 0.380\n",
            "feature:  0 corr: 0.384\n",
            "feature: 19 corr: 0.455\n",
            "feature: 21 corr: 0.531\n",
            "feature: 18 corr: 0.565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the features with the lowest correlation\n",
        "num = 17\n",
        "print(\"X shape before drops:\", X.shape)\n",
        "print(\"Corr shape:\", len(Corr))\n",
        "for i in range(num):\n",
        "  X = np.delete(X, Corr[i][1], 1)\n",
        "  # update feature positions\n",
        "  for j in Corr:\n",
        "    if j[1] > i:\n",
        "      j[1] = j[1] - 1\n",
        "print(\"X shape after drops:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFHDaIOdQxcq",
        "outputId": "a381a91b-9323-4cbf-d33d-e40f881583fd"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape before drops: (195, 22)\n",
            "Corr shape: 22\n",
            "X shape after drops: (195, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# partition into training and validation set at 80/20 division\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "# Converting y_r and y_t into one-hot encoding using a keras utility\n",
        "#  to_categorical\n",
        "\n",
        "# print(y_test)\n",
        "#[1, 0] is 0\n",
        "#[0, 1] is 1\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "# print(y_test)"
      ],
      "metadata": {
        "id": "1c48i7x0mjqC"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "if np.sum(np.isnan(X)):\n",
        "  print('Total of NaN before imputation:', np.sum(np.isnan(X)))\n",
        "  imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
        "  X1 = imputer.fit_transform(X)\n",
        "  print('Total of NaN after imputation:', np.sum(np.isnan(X1)))\n",
        "else:\n",
        "  X1 = X\n",
        "  print('no NaN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfIFa6bGkSOV",
        "outputId": "bb37b25c-fae0-4547-e479-c63fba71e923"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "print(\"before transformation\\n--------------------\")\n",
        "for i in range(X.shape[1]):\n",
        "  print(\"range of feature %i is %i\"%(i, np.ptp(X_train[:,i])))\n",
        "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-5, 5))\n",
        "X_train = min_max_scaler.fit_transform(X_train)\n",
        "X_test = min_max_scaler.fit_transform(X_test)\n",
        "print(\"after transformation\\n--------------------\")\n",
        "for i in range(X.shape[1]):\n",
        "  print(\"range of feature %i is %i\"%(i, np.ptp(X_train[:,i])))\n",
        "\n",
        "# standardize X_train and X_test\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "# X_train = scaler.transform(X_train)\n",
        "# X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjMxsi8Nkb3A",
        "outputId": "abf4ec26-befa-4c1f-b141-37ee48ef34ec"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before transformation\n",
            "--------------------\n",
            "range of feature 0 is 171\n",
            "range of feature 1 is 0\n",
            "range of feature 2 is 0\n",
            "range of feature 3 is 1\n",
            "range of feature 4 is 5\n",
            "after transformation\n",
            "--------------------\n",
            "range of feature 0 is 10\n",
            "range of feature 1 is 10\n",
            "range of feature 2 is 10\n",
            "range of feature 3 is 10\n",
            "range of feature 4 is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition"
      ],
      "metadata": {
        "id": "y7SOcpGbklkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(8, input_dim=5, activation = 'relu'),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.LayerNormalization(),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')])"
      ],
      "metadata": {
        "id": "UmfCUVYi5m6P"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the keras model\n",
        "net.compile(loss='binary_crossentropy', optimizer='Adadelta',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "HdkST6Om57mv"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "P2exBAUukoIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the keras model on the dataset\n",
        "net.fit(X_train, y_train, epochs=300, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQknY8D59ew",
        "outputId": "1deb1616-10d3-4451-8642-fac898f29598"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "16/16 [==============================] - 2s 6ms/step - loss: 1.3148 - accuracy: 0.2436\n",
            "Epoch 2/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2856 - accuracy: 0.2436\n",
            "Epoch 3/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2569 - accuracy: 0.2436\n",
            "Epoch 4/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.2284 - accuracy: 0.2436\n",
            "Epoch 5/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.2001 - accuracy: 0.2500\n",
            "Epoch 6/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.1725 - accuracy: 0.2500\n",
            "Epoch 7/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.1459 - accuracy: 0.2564\n",
            "Epoch 8/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.1198 - accuracy: 0.2564\n",
            "Epoch 9/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.0946 - accuracy: 0.3013\n",
            "Epoch 10/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.0703 - accuracy: 0.3205\n",
            "Epoch 11/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.0468 - accuracy: 0.3397\n",
            "Epoch 12/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 1.0246 - accuracy: 0.3654\n",
            "Epoch 13/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.0033 - accuracy: 0.3974\n",
            "Epoch 14/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.9832 - accuracy: 0.4103\n",
            "Epoch 15/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.9633 - accuracy: 0.4103\n",
            "Epoch 16/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.9447 - accuracy: 0.4231\n",
            "Epoch 17/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.9267 - accuracy: 0.4359\n",
            "Epoch 18/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9095 - accuracy: 0.4359\n",
            "Epoch 19/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8931 - accuracy: 0.4359\n",
            "Epoch 20/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8771 - accuracy: 0.4359\n",
            "Epoch 21/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8618 - accuracy: 0.4359\n",
            "Epoch 22/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8470 - accuracy: 0.4551\n",
            "Epoch 23/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8330 - accuracy: 0.4551\n",
            "Epoch 24/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8196 - accuracy: 0.4487\n",
            "Epoch 25/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8068 - accuracy: 0.4551\n",
            "Epoch 26/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7948 - accuracy: 0.5321\n",
            "Epoch 27/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7833 - accuracy: 0.5705\n",
            "Epoch 28/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7724 - accuracy: 0.5769\n",
            "Epoch 29/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7621 - accuracy: 0.5897\n",
            "Epoch 30/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.7520 - accuracy: 0.5897\n",
            "Epoch 31/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7423 - accuracy: 0.5962\n",
            "Epoch 32/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7331 - accuracy: 0.5962\n",
            "Epoch 33/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.7245 - accuracy: 0.6090\n",
            "Epoch 34/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7163 - accuracy: 0.6218\n",
            "Epoch 35/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.7084 - accuracy: 0.6346\n",
            "Epoch 36/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7011 - accuracy: 0.6346\n",
            "Epoch 37/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.6346\n",
            "Epoch 38/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6871 - accuracy: 0.6410\n",
            "Epoch 39/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.6474\n",
            "Epoch 40/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6740 - accuracy: 0.6538\n",
            "Epoch 41/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.6677 - accuracy: 0.6538\n",
            "Epoch 42/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6618 - accuracy: 0.6538\n",
            "Epoch 43/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6559 - accuracy: 0.6538\n",
            "Epoch 44/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6502 - accuracy: 0.6538\n",
            "Epoch 45/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6446 - accuracy: 0.6603\n",
            "Epoch 46/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6394 - accuracy: 0.6667\n",
            "Epoch 47/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6344 - accuracy: 0.6731\n",
            "Epoch 48/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6298 - accuracy: 0.6731\n",
            "Epoch 49/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6252 - accuracy: 0.6859\n",
            "Epoch 50/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6207 - accuracy: 0.6859\n",
            "Epoch 51/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6164 - accuracy: 0.6859\n",
            "Epoch 52/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6121 - accuracy: 0.6859\n",
            "Epoch 53/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.6080 - accuracy: 0.6987\n",
            "Epoch 54/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6040 - accuracy: 0.7051\n",
            "Epoch 55/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5999 - accuracy: 0.7115\n",
            "Epoch 56/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.7115\n",
            "Epoch 57/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5924 - accuracy: 0.7115\n",
            "Epoch 58/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5888 - accuracy: 0.7115\n",
            "Epoch 59/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5850 - accuracy: 0.7179\n",
            "Epoch 60/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5814 - accuracy: 0.7244\n",
            "Epoch 61/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5778 - accuracy: 0.7372\n",
            "Epoch 62/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5743 - accuracy: 0.7372\n",
            "Epoch 63/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5709 - accuracy: 0.7372\n",
            "Epoch 64/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5677 - accuracy: 0.7372\n",
            "Epoch 65/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5645 - accuracy: 0.7372\n",
            "Epoch 66/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5613 - accuracy: 0.7372\n",
            "Epoch 67/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5582 - accuracy: 0.7372\n",
            "Epoch 68/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5552 - accuracy: 0.7436\n",
            "Epoch 69/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5523 - accuracy: 0.7436\n",
            "Epoch 70/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5494 - accuracy: 0.7500\n",
            "Epoch 71/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5465 - accuracy: 0.7500\n",
            "Epoch 72/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5437 - accuracy: 0.7500\n",
            "Epoch 73/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5410 - accuracy: 0.7500\n",
            "Epoch 74/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5383 - accuracy: 0.7500\n",
            "Epoch 75/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7564\n",
            "Epoch 76/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5331 - accuracy: 0.7564\n",
            "Epoch 77/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5306 - accuracy: 0.7500\n",
            "Epoch 78/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5281 - accuracy: 0.7500\n",
            "Epoch 79/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.5256 - accuracy: 0.7500\n",
            "Epoch 80/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5233 - accuracy: 0.7564\n",
            "Epoch 81/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5208 - accuracy: 0.7628\n",
            "Epoch 82/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5185 - accuracy: 0.7692\n",
            "Epoch 83/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5163 - accuracy: 0.7692\n",
            "Epoch 84/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5140 - accuracy: 0.7692\n",
            "Epoch 85/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5119 - accuracy: 0.7756\n",
            "Epoch 86/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.7756\n",
            "Epoch 87/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5076 - accuracy: 0.7821\n",
            "Epoch 88/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5054 - accuracy: 0.7821\n",
            "Epoch 89/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5034 - accuracy: 0.7821\n",
            "Epoch 90/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.7821\n",
            "Epoch 91/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4994 - accuracy: 0.7821\n",
            "Epoch 92/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4974 - accuracy: 0.7821\n",
            "Epoch 93/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4955 - accuracy: 0.7821\n",
            "Epoch 94/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4936 - accuracy: 0.7821\n",
            "Epoch 95/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4917 - accuracy: 0.7821\n",
            "Epoch 96/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4898 - accuracy: 0.7821\n",
            "Epoch 97/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.7821\n",
            "Epoch 98/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.7821\n",
            "Epoch 99/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4844 - accuracy: 0.7885\n",
            "Epoch 100/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7949\n",
            "Epoch 101/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7949\n",
            "Epoch 102/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.7949\n",
            "Epoch 103/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.7949\n",
            "Epoch 104/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.7949\n",
            "Epoch 105/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4740 - accuracy: 0.7949\n",
            "Epoch 106/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4723 - accuracy: 0.7949\n",
            "Epoch 107/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4706 - accuracy: 0.8077\n",
            "Epoch 108/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.8077\n",
            "Epoch 109/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.8077\n",
            "Epoch 110/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4657 - accuracy: 0.8077\n",
            "Epoch 111/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.8077\n",
            "Epoch 112/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.8077\n",
            "Epoch 113/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4610 - accuracy: 0.8077\n",
            "Epoch 114/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8077\n",
            "Epoch 115/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4579 - accuracy: 0.8077\n",
            "Epoch 116/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4564 - accuracy: 0.8141\n",
            "Epoch 117/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4548 - accuracy: 0.8141\n",
            "Epoch 118/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4533 - accuracy: 0.8141\n",
            "Epoch 119/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4518 - accuracy: 0.8141\n",
            "Epoch 120/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4503 - accuracy: 0.8141\n",
            "Epoch 121/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.8141\n",
            "Epoch 122/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.8141\n",
            "Epoch 123/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4458 - accuracy: 0.8269\n",
            "Epoch 124/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4444 - accuracy: 0.8269\n",
            "Epoch 125/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4429 - accuracy: 0.8269\n",
            "Epoch 126/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4415 - accuracy: 0.8269\n",
            "Epoch 127/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.8269\n",
            "Epoch 128/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4386 - accuracy: 0.8269\n",
            "Epoch 129/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.8269\n",
            "Epoch 130/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.8269\n",
            "Epoch 131/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.8269\n",
            "Epoch 132/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4330 - accuracy: 0.8269\n",
            "Epoch 133/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4315 - accuracy: 0.8269\n",
            "Epoch 134/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4302 - accuracy: 0.8205\n",
            "Epoch 135/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4288 - accuracy: 0.8205\n",
            "Epoch 136/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4274 - accuracy: 0.8269\n",
            "Epoch 137/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4261 - accuracy: 0.8269\n",
            "Epoch 138/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4248 - accuracy: 0.8333\n",
            "Epoch 139/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4235 - accuracy: 0.8333\n",
            "Epoch 140/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4220 - accuracy: 0.8333\n",
            "Epoch 141/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4208 - accuracy: 0.8333\n",
            "Epoch 142/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4194 - accuracy: 0.8333\n",
            "Epoch 143/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4180 - accuracy: 0.8333\n",
            "Epoch 144/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.4168 - accuracy: 0.8269\n",
            "Epoch 145/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4155 - accuracy: 0.8333\n",
            "Epoch 146/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4142 - accuracy: 0.8269\n",
            "Epoch 147/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4129 - accuracy: 0.8269\n",
            "Epoch 148/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4117 - accuracy: 0.8269\n",
            "Epoch 149/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4105 - accuracy: 0.8269\n",
            "Epoch 150/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4092 - accuracy: 0.8333\n",
            "Epoch 151/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.4080 - accuracy: 0.8333\n",
            "Epoch 152/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4068 - accuracy: 0.8333\n",
            "Epoch 153/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4056 - accuracy: 0.8333\n",
            "Epoch 154/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4044 - accuracy: 0.8333\n",
            "Epoch 155/300\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4032 - accuracy: 0.8462\n",
            "Epoch 156/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4020 - accuracy: 0.8462\n",
            "Epoch 157/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4008 - accuracy: 0.8462\n",
            "Epoch 158/300\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3997 - accuracy: 0.8462\n",
            "Epoch 159/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3985 - accuracy: 0.8462\n",
            "Epoch 160/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3974 - accuracy: 0.8462\n",
            "Epoch 161/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3963 - accuracy: 0.8462\n",
            "Epoch 162/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3952 - accuracy: 0.8462\n",
            "Epoch 163/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3940 - accuracy: 0.8462\n",
            "Epoch 164/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3931 - accuracy: 0.8462\n",
            "Epoch 165/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3919 - accuracy: 0.8462\n",
            "Epoch 166/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3909 - accuracy: 0.8462\n",
            "Epoch 167/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8526\n",
            "Epoch 168/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 0.8526\n",
            "Epoch 169/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.8526\n",
            "Epoch 170/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8526\n",
            "Epoch 171/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3856 - accuracy: 0.8526\n",
            "Epoch 172/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3843 - accuracy: 0.8526\n",
            "Epoch 173/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3834 - accuracy: 0.8526\n",
            "Epoch 174/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3824 - accuracy: 0.8526\n",
            "Epoch 175/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3814 - accuracy: 0.8526\n",
            "Epoch 176/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3803 - accuracy: 0.8526\n",
            "Epoch 177/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 0.8526\n",
            "Epoch 178/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3783 - accuracy: 0.8526\n",
            "Epoch 179/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3773 - accuracy: 0.8526\n",
            "Epoch 180/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3763 - accuracy: 0.8526\n",
            "Epoch 181/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.8526\n",
            "Epoch 182/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3744 - accuracy: 0.8526\n",
            "Epoch 183/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3733 - accuracy: 0.8590\n",
            "Epoch 184/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3723 - accuracy: 0.8590\n",
            "Epoch 185/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3714 - accuracy: 0.8590\n",
            "Epoch 186/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3703 - accuracy: 0.8590\n",
            "Epoch 187/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3694 - accuracy: 0.8590\n",
            "Epoch 188/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3684 - accuracy: 0.8590\n",
            "Epoch 189/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3674 - accuracy: 0.8590\n",
            "Epoch 190/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3665 - accuracy: 0.8590\n",
            "Epoch 191/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3655 - accuracy: 0.8590\n",
            "Epoch 192/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3645 - accuracy: 0.8590\n",
            "Epoch 193/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3636 - accuracy: 0.8590\n",
            "Epoch 194/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.8590\n",
            "Epoch 195/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3617 - accuracy: 0.8590\n",
            "Epoch 196/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8590\n",
            "Epoch 197/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3598 - accuracy: 0.8590\n",
            "Epoch 198/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3588 - accuracy: 0.8590\n",
            "Epoch 199/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3579 - accuracy: 0.8590\n",
            "Epoch 200/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3570 - accuracy: 0.8590\n",
            "Epoch 201/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3561 - accuracy: 0.8590\n",
            "Epoch 202/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3551 - accuracy: 0.8590\n",
            "Epoch 203/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3542 - accuracy: 0.8654\n",
            "Epoch 204/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3533 - accuracy: 0.8654\n",
            "Epoch 205/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3525 - accuracy: 0.8654\n",
            "Epoch 206/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8654\n",
            "Epoch 207/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3507 - accuracy: 0.8654\n",
            "Epoch 208/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3498 - accuracy: 0.8654\n",
            "Epoch 209/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3489 - accuracy: 0.8654\n",
            "Epoch 210/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3480 - accuracy: 0.8654\n",
            "Epoch 211/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8654\n",
            "Epoch 212/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3463 - accuracy: 0.8654\n",
            "Epoch 213/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3454 - accuracy: 0.8654\n",
            "Epoch 214/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3446 - accuracy: 0.8654\n",
            "Epoch 215/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3437 - accuracy: 0.8654\n",
            "Epoch 216/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3429 - accuracy: 0.8654\n",
            "Epoch 217/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3421 - accuracy: 0.8654\n",
            "Epoch 218/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.8654\n",
            "Epoch 219/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3405 - accuracy: 0.8654\n",
            "Epoch 220/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3395 - accuracy: 0.8654\n",
            "Epoch 221/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3387 - accuracy: 0.8654\n",
            "Epoch 222/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8654\n",
            "Epoch 223/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3371 - accuracy: 0.8718\n",
            "Epoch 224/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3363 - accuracy: 0.8718\n",
            "Epoch 225/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3355 - accuracy: 0.8718\n",
            "Epoch 226/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3347 - accuracy: 0.8718\n",
            "Epoch 227/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.8718\n",
            "Epoch 228/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3332 - accuracy: 0.8718\n",
            "Epoch 229/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3325 - accuracy: 0.8718\n",
            "Epoch 230/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3316 - accuracy: 0.8718\n",
            "Epoch 231/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3308 - accuracy: 0.8718\n",
            "Epoch 232/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8718\n",
            "Epoch 233/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3292 - accuracy: 0.8718\n",
            "Epoch 234/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3285 - accuracy: 0.8718\n",
            "Epoch 235/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3277 - accuracy: 0.8718\n",
            "Epoch 236/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8718\n",
            "Epoch 237/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3263 - accuracy: 0.8718\n",
            "Epoch 238/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3254 - accuracy: 0.8718\n",
            "Epoch 239/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.8718\n",
            "Epoch 240/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3241 - accuracy: 0.8718\n",
            "Epoch 241/300\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8718\n",
            "Epoch 242/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3225 - accuracy: 0.8718\n",
            "Epoch 243/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8718\n",
            "Epoch 244/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8782\n",
            "Epoch 245/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8782\n",
            "Epoch 246/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8782\n",
            "Epoch 247/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8782\n",
            "Epoch 248/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3182 - accuracy: 0.8782\n",
            "Epoch 249/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8782\n",
            "Epoch 250/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3168 - accuracy: 0.8782\n",
            "Epoch 251/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3160 - accuracy: 0.8782\n",
            "Epoch 252/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8782\n",
            "Epoch 253/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8782\n",
            "Epoch 254/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8782\n",
            "Epoch 255/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8782\n",
            "Epoch 256/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3124 - accuracy: 0.8782\n",
            "Epoch 257/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3116 - accuracy: 0.8782\n",
            "Epoch 258/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3110 - accuracy: 0.8782\n",
            "Epoch 259/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3102 - accuracy: 0.8782\n",
            "Epoch 260/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3095 - accuracy: 0.8782\n",
            "Epoch 261/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3088 - accuracy: 0.8782\n",
            "Epoch 262/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3082 - accuracy: 0.8782\n",
            "Epoch 263/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3075 - accuracy: 0.8782\n",
            "Epoch 264/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3067 - accuracy: 0.8782\n",
            "Epoch 265/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3061 - accuracy: 0.8782\n",
            "Epoch 266/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3054 - accuracy: 0.8782\n",
            "Epoch 267/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3048 - accuracy: 0.8782\n",
            "Epoch 268/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3040 - accuracy: 0.8782\n",
            "Epoch 269/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3035 - accuracy: 0.8782\n",
            "Epoch 270/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8782\n",
            "Epoch 271/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8782\n",
            "Epoch 272/300\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3015 - accuracy: 0.8782\n",
            "Epoch 273/300\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3008 - accuracy: 0.8782\n",
            "Epoch 274/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3002 - accuracy: 0.8782\n",
            "Epoch 275/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2995 - accuracy: 0.8782\n",
            "Epoch 276/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2991 - accuracy: 0.8782\n",
            "Epoch 277/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2983 - accuracy: 0.8782\n",
            "Epoch 278/300\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2978 - accuracy: 0.8782\n",
            "Epoch 279/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2971 - accuracy: 0.8782\n",
            "Epoch 280/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2966 - accuracy: 0.8846\n",
            "Epoch 281/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2960 - accuracy: 0.8846\n",
            "Epoch 282/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2953 - accuracy: 0.8846\n",
            "Epoch 283/300\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2948 - accuracy: 0.8846\n",
            "Epoch 284/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2942 - accuracy: 0.8846\n",
            "Epoch 285/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2936 - accuracy: 0.8846\n",
            "Epoch 286/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2930 - accuracy: 0.8846\n",
            "Epoch 287/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2925 - accuracy: 0.8846\n",
            "Epoch 288/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2918 - accuracy: 0.8846\n",
            "Epoch 289/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2913 - accuracy: 0.8846\n",
            "Epoch 290/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2908 - accuracy: 0.8846\n",
            "Epoch 291/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2902 - accuracy: 0.8846\n",
            "Epoch 292/300\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2896 - accuracy: 0.8846\n",
            "Epoch 293/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2891 - accuracy: 0.8846\n",
            "Epoch 294/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2885 - accuracy: 0.8846\n",
            "Epoch 295/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2880 - accuracy: 0.8846\n",
            "Epoch 296/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2874 - accuracy: 0.8846\n",
            "Epoch 297/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2868 - accuracy: 0.8846\n",
            "Epoch 298/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2863 - accuracy: 0.8846\n",
            "Epoch 299/300\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2858 - accuracy: 0.8846\n",
            "Epoch 300/300\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2852 - accuracy: 0.8846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ec3444d3e50>"
            ]
          },
          "metadata": {},
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "JieNm8ZJkrYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model using keras built-in function\n",
        "score = net.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", format(score[0],\".4f\"))\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZY5YcnXgUWI",
        "outputId": "8303f192-e966-4e78-b681-cd359110e4f0"
      },
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.4671\n",
            "Test accuracy: 0.8461538553237915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predicted testing output\n",
        "# Since softmax output is used, these outputs are probability\n",
        "# vectors of value between 0 and 1 and values of each output\n",
        "# vector added to 1\n",
        "y_softmax = net.predict(X_test)\n",
        "# y_pc gives the indices (strating from 0) of the max elements\n",
        "y_pc = np.argmax(y_softmax, axis = -1)\n",
        "# convert y_pc into one-hot encoding\n",
        "y_pred = keras.utils.to_categorical(y_pc)\n",
        "\n",
        "Cmat = tf.math.confusion_matrix(y_test.argmax(axis=-1),y_pred.argmax(axis=-1))\n",
        "print(Cmat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LieYDryRZLXk",
        "outputId": "efd9186d-206e-4869-d334-c28ea9676884"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 10ms/step\n",
            "tf.Tensor(\n",
            "[[ 6  6]\n",
            " [ 0 27]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}